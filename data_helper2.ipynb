{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train')\n",
    "newsgroups_text = fetch_20newsgroups(subset = 'test')\n",
    "newsgroups_all = fetch_20newsgroups(subset = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts num in train: 11314\n",
      "texts num in test: 7532\n"
     ]
    }
   ],
   "source": [
    "print('texts num in train: %d'%(len(newsgroups_train.data)))\n",
    "print('texts num in test: %d'%(len(newsgroups_text.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. GANDLER)\n",
      "Subject: Need info on 88-89 Bonneville\n",
      "Organization: University at Buffalo\n",
      "Lines: 10\n",
      "News-Software: VAX/VMS VNEWS 1.41\n",
      "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
      "\n",
      "\n",
      " I am a little confused on all of the models of the 88-89 bonnevilles.\n",
      "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
      "differences are far as features or performance. I am also curious to\n",
      "know what the book value is for prefereably the 89 model. And how much\n",
      "less than book value can you usually get them for. In other words how\n",
      "much are they in demand this time of year. I have heard that the mid-spring\n",
      "early summer is the best time to buy.\n",
      "\n",
      "\t\t\tNeil Gandler\n",
      "\n",
      "label 7\n"
     ]
    }
   ],
   "source": [
    "print('context',newsgroups_text.data[0])\n",
    "print('label',newsgroups_text.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load w2v over\n",
      "200\n",
      "47135\n"
     ]
    }
   ],
   "source": [
    "w2v_cropus_file = './corpus/corpus_text8.txt'\n",
    "\n",
    "def loadword2vec(filename):\n",
    "    '''\n",
    "    @param w2v_file: w2v file path\n",
    "    @return word2vec_dict: a dictionary includes word and corresponding vector{word:vector}\n",
    "    @return word_dim: dimension of word\n",
    "    '''\n",
    "    vocab = []\n",
    "    embed = []\n",
    "    fr = open(filename,'r')\n",
    "    line = fr.readline().strip()\n",
    "    \n",
    "    word_dim = int(line.split(' ')[1])\n",
    "    vocab.append('UNK')\n",
    "    embed.append([0]*word_dim)\n",
    "    \n",
    "    for line in fr:\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embed.append(row[1:])\n",
    "    \n",
    "    print('load w2v over')\n",
    "    fr.close()\n",
    "    return vocab,embed\n",
    "\n",
    "vocab,embed = loadword2vec(w2v_cropus_file)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embed[0])\n",
    "\n",
    "\n",
    "print(embedding_dim)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721\n"
     ]
    }
   ],
   "source": [
    "print(len(newsgroups_train.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    int_to_vocab = {}\n",
    "    vocab_to_int = {}\n",
    "    for word,i in enumerate(text):\n",
    "        int_to_vocab[i],vocab_to_int[word] = word,i\n",
    "    \n",
    "    return int_to_vocab,vocab_to_int\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_data(string):\n",
    "    import re\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflearn.data_utils import pad_sequences\n",
    "\n",
    "vocabulary_word2index,vocabulary_index2word = create_lookup_tables(vocab) \n",
    "\n",
    "X = []\n",
    "#train  clear->word2index->list append\n",
    "for i,sentence in enumerate(newsgroups_train.data):\n",
    "   \n",
    "    sentence = clear_data(sentence) \n",
    "    s = sentence.split(\" \")  \n",
    "    x = [vocabulary_word2index.get(w,vocabulary_word2index['UNK']) for w in s]\n",
    "    X.append(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "11314\n",
      "[28, 0, 0, 0, 7711, 0, 100, 0, 609, 1798, 0, 576, 154, 983, 11, 33, 0, 0, 0, 12631, 1560, 0, 0, 0, 7711, 709, 127, 2, 4152, 0, 354, 787, 823, 0, 72, 18, 29818, 88, 2615, 108, 60, 160, 0, 746, 25, 33, 983, 72, 798, 1, 43, 137, 27, 18, 6, 0, 2931, 1327, 983, 0, 4163, 7, 32, 28, 1, 329, 0, 128, 0, 27, 18, 89, 6, 40212, 1, 4379, 40, 2269, 221, 5, 536, 0, 1, 927, 35118, 18, 834, 28, 1, 995, 2, 1, 341, 33, 11, 50, 72, 1006, 88, 2615, 53, 0, 6, 518, 121, 0, 1030, 39191, 0, 81, 2, 443, 0, 100, 33, 983, 11, 109, 0, 96, 0, 29, 3537, 3862, 207, 39, 25, 33, 21423, 2398, 983, 0, 3877, 112, 2214, 4414, 0, 2796, 754, 7, 207, 19, 772, 5931, 0]\n",
      "721\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))\n",
    "print(len(X))\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    save&padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21273.0\n",
      "20.0\n",
      "360.608538094\n",
      "209.0\n",
      "574.0\n"
     ]
    }
   ],
   "source": [
    "#max sentence length\n",
    "len_counter = np.zeros(len(X))\n",
    "for i,w in enumerate(X):\n",
    "    len_counter[i] = len(w)\n",
    "\n",
    "print(np.max(len_counter))\n",
    "print(np.min(len_counter))\n",
    "print(np.mean(len_counter))\n",
    "print(np.median(len_counter))\n",
    "print(np.percentile(len_counter,90))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_length = np.percentile(len_counter,90)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
